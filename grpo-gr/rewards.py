"""Reward functions for GRPO-GR training."""

import re


import numpy as np
import json
import re
from collections import defaultdict

import nltk
from nltk.translate.bleu_score import sentence_bleu

from callGPT import call_gpt


def gpt_score_reward(prompts, completions, gt_answer, **kwargs):
    """Reward based on gpt-4o evaluation between generated response and correct answer."""
    rewards = []
    # pattern = r'<answer>(.*?)<answer_end>'  # generated by chatGPT
    for i in range(len(gt_answer)):
        if 'dataset' in kwargs and 'ovd' in kwargs['dataset'][i]:
            rewards.append(0.0)
            continue
        response, answer = completions[i], gt_answer[i]
        question = prompts[i][0]['content'][-1]['text'].split('Please answer question: ')[-1]

        reward = 0.0
        # match_pattern = response.split('<answer>', 1)[-1]#re.findall(pattern, response, re.DOTALL) #response.split('<response>')[-1].split('<think_end>')[-1] 
        if '<answer>' in response:
            match_pattern = response.split('<answer>', 1)
            predicted_content = match_pattern[-1].split('</answer>')[0]
            prompt = f"""You are responsible for proofreading the answers, you need to give a score to the model’s answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the json form "{{score: <score>}}". The evaluation criteria require that the closer the model’s answer is to the standard answer, the higher the score.
Question: {question} 
Standard answer: {answer} 
Model’s answer: {predicted_content}"""

            tempt = 0
            while tempt <3:
                tempt+=1
                try:
                    output = re.sub(r'[^\w\s\.]', '', call_gpt(prompt).strip().lower())
                    output_num = output.split("score")[-1][:5]
                    
                    # remove any text that is not a number 
                    output = re.sub(r'[^\d.]', '', output_num)
                    output = float(output)
                    break
                except Exception as e:
                    print("Error in gpt_score_reward: ", e)
                    output =  0
            reward += output
        rewards.append(reward)
    
    return rewards


def gpt_score_reward_1(prompts, completions, gt_answer, **kwargs):
    """Reward based on gpt-4o evaluation between generated response and correct answer."""
    rewards = []
    # pattern = r'<answer>(.*?)<answer_end>'  # generated by chatGPT
    for i in range(len(gt_answer)):
        response, answer = completions[i], gt_answer[i]
        question = prompts[i][0]['content'][-1]['text'].split('Please answer question: ')[-1]

        reward = 0.0
        
        predicted_content = response.strip()
        prompt = f"""You are responsible for proofreading the answers, you need to give a score to the model’s answer by referring to the standard answer, based on the given question. The full score is 1 point and the minimum score is 0 points. Please output the score in the json form "{{score: <score>}}". The evaluation criteria require that the closer the model’s answer is to the standard answer, the higher the score.
Question: {question} 
Standard answer: {answer} 
Model’s answer: {predicted_content}"""
        if response != '':
            tempt = 0
            while tempt <3:
                tempt+=1
                try:
                    output = re.sub(r'[^\w\s\.]', '', call_gpt(prompt).strip().lower())
                    output_num = output.split("score")[-1][:5]
                    
                    # remove any text that is not a number 
                    output = re.sub(r'[^\d.]', '', output_num)
                    output = float(output)
                    break
                except Exception as e:
                    print("Error in gpt_score_reward: ", e)
                    output =  0
            
            reward += output
        rewards.append(reward)
    
    return rewards



def bleu_score_reward(prompts, completions, gt_answer, **kwargs):
    """Reward based on BLEU score between generated response and correct answer."""
    rewards = []
    max_reward = 0.1
    # pattern = r'<answer_start>(.*?)<answer_end>'  # generated by chatGPT
    for i in range(len(gt_answer)):
        response, answer = completions[i], gt_answer[i]
        question = prompts[i][0]['content'][-1]['text'].split('Please answer question: ')[-1]

        reward = 0.0
        # match_pattern = re.findall(pattern, response, re.DOTALL) # re.findall(pattern, response, re.DOTALL)
        
        
        if '<answer>' in response:
            match_pattern = response.split('<answer>', 1)
            predicted_content = match_pattern[-1]
            
            predicted_content = re.sub(r'[^a-zA-Z0-9\s]', ' ', predicted_content)
            answer = re.sub(r'[^a-zA-Z0-9\s]', ' ', answer)
            reward += sentence_bleu([answer.lower().split()], predicted_content.lower().split(), weights=(1, 0, 0, 0))  
        rewards.append(reward * max_reward)
    return rewards

def answer_format_reward(prompts, completions, gt_answer, **kwargs):
    """Reward if generated response contains correct answer."""
    rewards = []
    max_reward = 0.5
    keys = ["<answer>"] #  'Result:', '<submit>' '<request><SimpleCalculatorTool>', '<call>', '<response>', 
    for response, answer in zip(completions, gt_answer):
        reward = 0.0
        for key in keys:
            if key in response:
                reward += 1.0
                if len(response.split(key)) == 2:
                    reward += 1.0
        rewards.append(reward/len(keys)/2*max_reward)

    return rewards


def think_and_rethink_format_reward(prompts, completions, gt_answer, **kwargs):
    """Reward if generated response contains correct answer."""
    rewards = []
    max_reward = 0.5
    keys = ["<think>", "</think>", "<rethink>", "</rethink>"] #  'Result:', '<submit>' '<request><SimpleCalculatorTool>', '<call>', '<response>', 
    for response, answer in zip(completions, gt_answer):
        reward = 0.0
        response_original = response
        for key in keys:
            if key in response:
                reward += 1.0
                # if len(response.split(key)) == 2:
                #     reward += 1.0
                response = response.split(key)[-1]
        if reward == len(keys):
            #check if thinking content is repeating
            try:
                response_list = response_original.split("<think>")[-1].split("</think>")[0].strip()
                assert isinstance(response_list, str) and len(response_list)>1 # at least 2 letters
                response_list = re.sub(r'[^a-zA-Z0-9\s]', ' ', response_list)
                response_list = response_list.split(' ')
                assert len(response_list)>1 # at least 2 words
                reward += 1.0
            except Exception as e:
                print("Error in think_format_reward: ", e)
        rewards.append(reward/(len(keys)+1)*max_reward) 

    return rewards



def grounded_region_specific_thinking_format_reward_think_rethink(prompts, completions, gt_answer, **kwargs):
    rewards = []
    for response, answer in zip(completions, gt_answer):
        if not '<rethink>' in response:
            reward = 0.0
            rewards.append(reward)
            continue
            
        response = response.split("<rethink>")[0]
        reward = 0.0
        pattern = r'\b\d+,\s*\d+,\s*\d+,\s*\d+\b'
        matches = re.findall(pattern, response)
                
        if len(matches) > 0:
            reward += 0.5
        if kwargs['dataset'][0] == 'tallyqa' and len(matches) == int(answer):
            reward += 1

                
        rewards.append(reward) 

    return rewards

def zipngram(text: str, ngram_size: int):
    words = text.lower().split()
    return zip(*[words[i:] for i in range(ngram_size)])

def repetitive_reward(prompts, completions, completion_ids, gt_answer, **kwargs):
    """Reward function that checks if the completion has repeting workds."""
    ngram_size = 8
    max_reward = 0.5
    rewards = []
    pad_token_id = 151643
    # iterate over each generated sequence in the batch
    for ids, completion in zip(completion_ids, completions):
        
        # for words repeating
        
        if completion == "":
            rewards.append(max_reward)
            continue
        if len(completion.split()) < ngram_size:
            rewards.append(max_reward)
            continue
        repeat_count = 0
        total = 0
        tokens = completion.split()


        for i in range(len(tokens) - ngram_size):
            ng1 = tuple(tokens[i:i+ngram_size])
            ng2 = tuple(tokens[i+ngram_size:i+ngram_size+ngram_size])
            total += 1
            if ng1 == ng2:
                repeat_count += 1

        if total == 0:
            reward = 1.0
        else:
            reward = 1.0 - (repeat_count / total)
        
        
        
        
        # convert to Python list
        ids_list = ids.tolist()
        # drop everything from the first pad_token_id onward
        if pad_token_id is not None:
            if pad_token_id in ids_list:
                ids_list = ids_list[: ids_list.index(pad_token_id)]
        # if too short to form two full n‑grams, give full reward
        if len(ids_list) < 2 * ngram_size:
            rewards.append(max_reward)
            continue

        repeat_count = 0
        total_pairs = 0
        # compare each n‑gram with the immediately following one
        for i in range(len(ids_list) - 2*ngram_size + 1):
            ng1 = tuple(ids_list[i : i + ngram_size])
            ng2 = tuple(ids_list[i + ngram_size : i + 2*ngram_size])
            total_pairs += 1
            if ng1 == ng2:
                repeat_count += 1

        # if there were no valid comparisons, treat as non‑repetitive
        if total_pairs == 0:
            score = 1.0
        else:
            score = 1.0 - (repeat_count / total_pairs)

        # scale into [0, max_reward]
        rewards.append((score - (1-reward)) * max_reward)
    return rewards
